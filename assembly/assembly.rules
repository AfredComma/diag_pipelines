import re
import csv
from snakemake.utils import report

shell.prefix("set -euo pipefail;") 

rule mlst:
    conda:
        "env/mlst.yaml"
    input:
        "{sample}/annotation/{sample}.fsa"
    output:
        "{sample}/mlst/mlst.out"
    shell:
        "mlst {input[0]} > {output[0]}"

rule trimmomatic:
    conda:
        "env/trim.yaml"
    input:
        "reads/raw/{sample}_R1_001.fastq",
        "reads/raw/{sample}_R2_001.fastq"
    params:
        minlength = config["minimum_read_length"],
        minqual = config["minimum_quality_base"],
        croplength = config["crop_at_read_beginning"]
    output:
        "{sample}/reads/trimmed/R1_paired.fastq",
        "{sample}/reads/trimmed/R1_unpaired.fastq",
        "{sample}/reads/trimmed/R2_paired.fastq",
        "{sample}/reads/trimmed/R2_unpaired.fastq",
        "{sample}/logs/trimmomatic.log"
    shell:
        "trimmomatic PE {input} {output[0]} {output[1]} {output[2]} {output[3]}  ILLUMINACLIP:TruSeq3-PE.fa:3:25:6 LEADING:{params.minqual} TRAILING:{params.minqual} MINLEN:{params.minlength} HEADCROP:{params.croplength} &> {output[4]}"

rule identification:
    conda:
        "env/kraken.yaml"
    params:
        kraken_db=config["kraken_path"]
    input:
        "{sample}/reads/trimmed/R1_paired.fastq",
        "{sample}/reads/trimmed/R2_paired.fastq"
    output:
        "{sample}/logs/kraken.log"
    shell:
        """
        export KRAKEN_DEFAULT_DB="{params.kraken_db}"
        kraken --paired {input[0]} {input[1]} | kraken-report | awk '$4=="S"' | sort -r | head -n 5 | cut -f1,6 | sed "s/^ *//" | sed "s/  \+/\\t/g" > {output[0]}
        """
        
rule virulence_factor_search:
    input:
        "{sample}/annotation/{sample}.faa"
    output:
        "{sample}/virulence/"
    shell:
        ""

rule fastqc:
    conda:
        "env/fastqc.yaml"
    input:
        "{sample}/reads/trimmed/R1_paired.fastq",
        "{sample}/reads/trimmed/R2_paired.fastq"
    output:
        "{sample}/reads/trimmed/fastqc/R1_paired_fastqc.zip",
        "{sample}/reads/trimmed/fastqc/R2_paired_fastqc.zip",
        "{sample}/reads/trimmed/fastqc/R1_paired_fastqc.html",
        "{sample}/reads/trimmed/fastqc/R2_paired_fastqc.html"
    shell:
        "fastqc {input} -o $( dirname {output[0]})"
    
rule unzip_fastqc:
    input:
        "{sample}/reads/trimmed/fastqc/R1_paired_fastqc.zip",
        "{sample}/reads/trimmed/fastqc/R2_paired_fastqc.zip"
    output:
        "{sample}/reads/trimmed/fastqc/R1_paired_fastqc/summary.txt",
        "{sample}/reads/trimmed/fastqc/R2_paired_fastqc/summary.txt"
    shell:
        "unzip -D -u {input[0]} -d $( dirname {input[0]}) && unzip -D -u {input[1]} -d $( dirname {input[0]}) && touch {output[0]} &&  touch {output[1]}"

        
rule error_correction_spades:
    conda:
        "env/spades.yaml"
    input:
        "{sample}/reads/trimmed/R1_paired.fastq",
        "{sample}/reads/trimmed/R2_paired.fastq"
    output:
        "{sample}/reads/corrected/R1_paired.00.0_0.cor.fastq.gz",
        "{sample}/reads/corrected/R2_paired.00.0_0.cor.fastq.gz"
    shell:
         "spades.py -1 {input[0]} -2 {input[1]} --only-error-correction -o $( dirname $(dirname {output[0]}))"
        
rule check_low_coverage_contigs:
    input:
        "{sample}/assembly/spades/contigs_500bp_low_kmer_coverage.fasta"
    output:
        "{sample}/blast_check/contig_low_kmer_coverage.txt"
    shell:
        "blastn -query {input[0]} -db dub -remote"
        
rule spades:
    conda:
        "env/spades.yaml"
    input:
        "{sample}/reads/corrected/R1_paired.00.0_0.cor.fastq.gz",
        "{sample}/reads/corrected/R2_paired.00.0_0.cor.fastq.gz"
    output:
        "{sample}/assembly/spades/spades.log",
        "{sample}/assembly/spades/contigs.fasta"
    shell:
        "spades.py --only-assembler -1 {input[0]} -2 {input[1]} -o $( dirname {output[0]})"

rule quast:
    conda:
        "env/quast.yaml"
    input:
        "{sample}/assembly/spades/contigs_500bp_high_kmer_coverage.fasta"
    output:
        "{sample}/assembly/spades/quast/report.txt"
    shell:
        "quast.py {input} -o $( dirname {output})"

rule extract_contigs_500bp:
    input:
        "{sample}/assembly/spades/contigs.fasta"
    output:
        "{sample}/assembly/spades/contigs_500bp.fasta"
    shell:
        "awk '/^>/{{print (NR==1)?$0: \"\\n\" $0;next}} {{printf \"%s\", $0}}END{{print \"\"}}' {input} |  awk \'!/^>/ {{ next }} {{ getline seq }} length(seq) >= 500 {{ print $0 \"\\n\" seq }}\'  > {output}"  


rule rename_contigs:
    input:
        "{sample}/assembly/spades/contigs_500bp_high_kmer_coverage.fasta"
    output:
        "{sample}/assembly/spades/contigs_500bp_high_kmer_coverage_renamed.fasta"
    shell:
        "sed \"s/NODE_\\([0-9]\\+\\)_.*/contig00\\1/\" {input} > {output}"
        
rule prokka:
    conda:
        "env/prokka.yaml"
    input:
        "{sample}/assembly/spades/contigs_500bp_high_kmer_coverage_renamed.fasta"
    output:
        "{sample}/annotation/{sample}.log",
        "{sample}/annotation/{sample}.gff",
        "{sample}/annotation/{sample}.faa",
        "{sample}/annotation/{sample}.fsa",
        "{sample}/annotation/{sample}.txt"
    shell:
        "prokka --outdir $( dirname {output[0]}) --force {input} --prefix {wildcards.sample}"


rule check_coverage_assembly:
    input:
        "{sample}/assembly/spades/contigs_500bp.fasta"
    output:
        "{sample}/assembly/spades/contigs_500bp_low_kmer_coverage.txt",
        "{sample}/assembly/spades/contigs_500bp_high_kmer_coverage.txt"
    shell:
        "grep \">\" {input} | sed \"s/.*cov_//\" | awk '$1 < 5 {{print NR}}' | sed \"s/^/NODE_/\" | sed \"s/$//\" | sed \"s/^>//\" > {output[0]} && grep \">\" {input} | sed \"s/.*cov_//\" | awk '$1 > 5 {{print NR}}' | sed \"s/^/NODE_/\" | sed \"s/$/_/\" | sed \"s/^>//\"  > {output[1]} "


rule filter_contigs_on_coverage:
    input:
        "{sample}/assembly/spades/contigs_500bp.fasta",
        "{sample}/assembly/spades/contigs_500bp_high_kmer_coverage.txt",
        "{sample}/assembly/spades/contigs_500bp_low_kmer_coverage.txt"
    output:
        "{sample}/assembly/spades/contigs_500bp_high_kmer_coverage.fasta",
        "{sample}/assembly/spades/contigs_500bp_low_kmer_coverage.fasta"
    shell:
        """
        grep -A 1 -f {input[1]} {input[0]} | sed '/^--$/d' > {output[0]}
        if [ -f {input[2]} ]
        then 
            grep -A 1 -f {input[2]} {input[0]} | sed '/^--$/d' > {output[1]}
        else
            touch {output[1]}
        fi
        """

rule convert_quast_images:
    conda:
        "env/imagemagick.yaml"
    input:
        "{sample}/assembly/spades/quast/report.txt"
    output:
        "{sample}/assembly/spades/quast/basic_stats/contigs_500bp_high_kmer_coverage_GC_content_plot.png",
        "{sample}/assembly/spades/quast/basic_stats/contigs_500bp_high_kmer_coverage_coverage_histogram.png"
    shell:
        """
        convert $(dirname {input[0]})/basic_stats/contigs_500bp_high_kmer_coverage_coverage_histogram.pdf $(dirname {input[0]})/basic_stats/contigs_500bp_high_kmer_coverage_coverage_histogram.png
        convert $(dirname {input[0]})/basic_stats/contigs_500bp_high_kmer_coverage_GC_content_plot.pdf $(dirname {input[0]})/basic_stats/contigs_500bp_high_kmer_coverage_GC_content_plot.png
        """

        
    
rule gff_to_embl:
    conda:
        "env/gff3toembl.yaml"
    input:
        "{sample}/annotation/{sample}.gff"
    output:
        "{sample}/ena_submission/flatfile.embl"
    params:
        publ=config["publication"],
        id=config["taxon_id_number"],
        name=config["taxon_name"],
        PRJ=config["EBI_project_number"],
        desc=config["description_of_the_genome_sequence"],
        aut="'"+" ".join(config["authors"])+"'",
        tt=config["translation_table"],
        pre=config["prefix_locus_tag"]
    shell:
        "gff3_to_embl --translation_table {params.tt} --locus_tag {params.pre} --authors {params.aut} --classification PROK --publication {params.publ} --output_filename {output} {params.name} {params.id} {params.PRJ} {params.desc} {input} "

rule validate_gff_for_ena_submission:
    conda:
        "env/validator_ena.yaml"
    input:
        "{sample}/ena_submission/flatfile.embl"
    output:
        "{sample}/ena_submission/VAL_SUMMARY.txt",
        "{sample}/ena_submission/VAL_ERROR.txt"
        
    shell:
        "cd $( dirname {input}) && embl-api-validator $( basename {input}) || true "
        

def extract_report(filename, pattern):
    s=''
    with open(filename) as text:
        for i, line in enumerate(text):
            if pattern.search(line) is not None:
                s=s+line+"\n"
    if s.isspace():
        return "    "
    return s

def extract_report_quast(filename):
    with open(filename) as text:
        l = text.readlines()
        return "\n".join(l[-9:-1])

def extract_report_trimmomatic(filename):
    with open(filename) as f:
        lines = f.read().splitlines()
        return [int(s) for s in lines[-2].split() if s.isdigit()]

def format_table(string):
    s = "    "
    for i in string.split("\n"):
        if len(i):
            s = s + "\t".join(i.split("\t")[0:2]) + "\n    " 
    return(s)

def extract_info_quast(l, index):
    return [x for x in l[index].split(" ") if x is not ""][-1]

def extract_info_prokka(filename):
    with open(filename) as f:
        d = {}
        for row in csv.reader(f, delimiter=":"):
            d[row[0]] = row[1]
        return(d)

rule report:
    input:
        trim = "{sample}/logs/trimmomatic.log",
        fastqc1 = "{sample}/reads/trimmed/fastqc/R1_paired_fastqc/summary.txt",
        fastqc2 = "{sample}/reads/trimmed/fastqc/R2_paired_fastqc/summary.txt",
        fastqc1html = "{sample}/reads/trimmed/fastqc/R1_paired_fastqc.html",
        fastqc2html = "{sample}/reads/trimmed/fastqc/R2_paired_fastqc.html",
        spades = "{sample}/assembly/spades/spades.log",
        low_cov_names = "{sample}/assembly/spades/contigs_500bp_low_kmer_coverage.txt",
        low_cov = "{sample}/assembly/spades/contigs_500bp_low_kmer_coverage.fasta",        
        quast = "{sample}/assembly/spades/quast/report.txt",
        prokka = "{sample}/annotation/{sample}.txt",
        ena_validator_summary = "{sample}/ena_submission/VAL_SUMMARY.txt",
        ena_validator_error = "{sample}/ena_submission/VAL_ERROR.txt",
        cov_plot = "{sample}/assembly/spades/quast/basic_stats/contigs_500bp_high_kmer_coverage_coverage_histogram.png",
        GC_plot = "{sample}/assembly/spades/quast/basic_stats/contigs_500bp_high_kmer_coverage_GC_content_plot.png",
        kraken = "{sample}/logs/kraken.log"
    output:
        "{sample}_report.html"
    run:
        trim_result = extract_report_trimmomatic(input["trim"])
        trim_result_percent = [ x / float(trim_result[0]) for x in trim_result ]
        trim_result_percent_strings = [ "{:2.1f}".format(x*100) for x in trim_result_percent ]
        warn = re.compile(r"WARN")
        fail = re.compile(r"FAIL")
        error = re.compile(r"ERROR")
        test2 = extract_report(input["fastqc2"], fail)
        assembly = extract_report_quast(input["quast"]).split("\n")
        ncontigs = extract_info_quast(assembly, 0)
        largest_contig = extract_info_quast(assembly, 2)
        total_length = extract_info_quast(assembly, 4)
        gc = extract_info_quast(assembly, 6)
        n50 = extract_info_quast(assembly, 8)
        r1f = format_table(extract_report(input["fastqc1"], fail))
        r1w = format_table(extract_report(input["fastqc1"], warn))
        r2f = format_table(extract_report(input["fastqc2"], fail))
        r2w = format_table(extract_report(input["fastqc2"], warn))
        spw = extract_report(input["spades"], warn)
        spw = "    " + "\n    ".join([ x for x in spw.split("\n") if "*" in x ])
        if spw.isspace():
            spw = "    No warnings nor errors"
        contigs_excluded = "    "+"    ".join(open(input["low_cov_names"]).readlines()).replace("_", " ")
        if contigs_excluded.isspace():
            contigs_excluded = "    No low kmer coverage contig"
        kraken = "    "+"     ".join(open(input["kraken"]).readlines()).replace("\t\t", " ")
        prokka_annot = extract_info_prokka(input["prokka"])
        rRNA = prokka_annot["rRNA"]
        CDS = prokka_annot["CDS"]
        tRNA = prokka_annot["tRNA"]
        enavalidator_warn = format_table(extract_report(input["ena_validator_summary"], warn))
        enavalidator_err = format_table(extract_report(input["ena_validator_summary"], error))
        cov_plot=input["cov_plot"]
        GC_plot = input["GC_plot"]
        report("""
        Report of the assembly and annotation of the strain {wildcards.sample}
        ===========================================================================================
        Reads were trimmed using Trimmomatic.\n
        ====================================== =================================
        Total number of pairs                  {trim_result[0]}
        Total of pairs passing trimming        {trim_result[1]} ({trim_result_percent_strings[1]} %)
        Forward read only surviving            {trim_result[2]} ({trim_result_percent_strings[2]} %)
        Reverse read only surviving            {trim_result[3]} ({trim_result_percent_strings[3]} %)
        Both reads dropped                     {trim_result[4]} ({trim_result_percent_strings[4]} %)
        ====================================== =================================
        
        (File trim_)\n
        The quality check on the pairs with both reads passing trimming was performed using fastqc. Fastqc reported the following warnings and failures:\n
        For forward reads::        
            
        {r1w}
        {r1f}
        
        (File fastqc1html_) \n

        For reverse reads::

        {r2w}
        {r2f}
        
        (File fastqc2html_) \n
        Check the html fastqc files for the details of the quality checks.

        Read classification per species (in %)::

        {kraken}
        
        The genome was assembled using spades. Spades reported the following warnings::
        
        {spw}
        
        (File spades_) \n

        The contigs were filtered based on their kmer coverage and those that had too low coverage (less than 5) were excluded from the analysis. The excluded contigs were::
        
        {contigs_excluded}
        
        Try blasting their sequences (in file low_cov_) to check their origin.
        
        The statistics of the assembly, without the excluded contigs, were obtained with quast: 
        
        ================================= ==============================
        Number of contigs                 {ncontigs}
        Largest contig                    {largest_contig} bps
        Total length                      {total_length} bps
        GC content                        {gc} %
        N50                               {n50} bps
        ================================= ==============================
        
        The histogram for GC content and kmer coverage can indicate contamination if outlier values are observed:
        
        .. image:: {cov_plot}
        .. image:: {GC_plot}

        Genes were annotated using prokka in local. 

        ================================= ==============================
        Number of CDS                     {CDS}
        Number of rRNA genes              {rRNA}
        Number of tRNA genes              {tRNA}
        ================================= ==============================
        
        The European Nucleotide Archive (ENA) submission file was prepared and checked using their validator. The reports of the validator were::

        {enavalidator_warn}
        {enavalidator_err}

        Check files ena_validator_summary_ and ena_validator_error_
 
        """, output[0], **input)
