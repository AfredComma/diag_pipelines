import re
from snakemake.utils import report



configfile: "comparative_search.yaml"

genus = config["search"]
genus_query = "|".join(genus)

shell.prefix("set -euo pipefail;")


samples="".join(open("ids.txt").readlines()).split("\n")[:-1]

print(samples)

rule download_genomes_from_refseq:
    input:
        "refSeq/genome_addresses.txt"
    output:
        "refSeq/genomes/downloads.txt"
    shell:
        """
        while read p;
        do  
            id=$(echo ${{p}} | sed "s/ ftp:.*//" | tr " " "_" | tr "\\t" "_" | tr "." "_")
            address=$(echo ${{p}} | sed "s/.* //" | sed \"s/\(\/GCF_.*\)/\\1\\1_genomic.fna.gz/\")
            echo ${{id}} ${{address}} >> {output}
            wget -qO- ${{address}} | gzip -d > refSeq/genomes/${{id}}.fna 
        done < {input}
        """

        
rule download_proteomes_from_refseq:
    input:
        "refSeq/genome_addresses.txt"
    output:
        "refSeq/proteomes/downloads.txt"
    shell:
        """
        while read p;
        do  
            id=$(echo ${{p}} | sed "s/ ftp:.*//" | tr " " "_" | tr "\\t" "_" | tr "." "_")
            address=$(echo ${{p}} | sed "s/.* //" | sed \"s/\(\/GCF_.*\)/\\1\\1_protein.faa.gz/\")
            echo ${{id}} ${{address}} >> {output}
            wget -qO- ${{address}} | gzip -d > refSeq/proteomes/${{id}}.faa 
        done < {input}
        """

rule extract_genome_refseq_info:
    conda:
        "env/entrez-direct.yaml"
    input:
        "comparative_search.yaml"
    output:
        "refSeq/nb_genomes_per_species.txt"
    shell:
        "esearch -db assembly -query \"{genus_query}\" | efetch -db assembly -format docsum | xtract -pattern DocumentSummary -if RefSeq -element SpeciesName  | sort | uniq -c | sort -k 1 -n > {output}"


rule fetch_refSeq_id_and_filtering:
    conda:
        "env/entrez-direct.yaml"
    input:
        "refSeq/nb_genomes_per_species.txt"
    output:
        "refSeq/genome_addresses.txt"
    shell:
        """
        awk '$1 > 5' {input} | sed "s/^\s*[0-9]\+//" | sed "s/\[//"  | sed "s/\]//"  | xargs -I{{}} sh -c 'esearch -db assembly -query \"$1\" | efetch -db assembly -format docsum | xtract -pattern DocumentSummary -unless RefSeq_category -equals \"na\" -if RefSeq -if Sub_value -element SpeciesName Sub_value RefSeq FtpPath_RefSeq >> {output}' -- {{}}
        awk '$1 > 5' {input} | sed "s/^\s*[0-9]\+//" | sed "s/\[//"  | sed "s/\]//"  | xargs -I{{}} sh -c 'esearch -db assembly -query \"$1\" | efetch -db assembly -format docsum | xtract -pattern DocumentSummary -unless RefSeq_category -equals \"na\" -if RefSeq -unless Sub_value -element SpeciesName Sub_value RefSeq FtpPath_RefSeq | sed "s/GCF_/NA\tGCF_/" >> {output}' -- {{}}
        awk '$1 < 5' {input} | sed "s/^\s*[0-9]\+//" | sed "s/\[//"  | sed "s/\]//" |  xargs -I{{}} sh -c 'esearch -db assembly -query \"$1\" | efetch -db assembly -format docsum | xtract -pattern DocumentSummary -if RefSeq -if Sub_value -element SpeciesName Sub_value RefSeq FtpPath_RefSeq >> {output}' -- {{}}
        awk '$1 < 5' {input} | sed "s/^\s*[0-9]\+//" | sed "s/\[//"  | sed "s/\]//" |  xargs -I{{}} sh -c 'esearch -db assembly -query \"$1\" | efetch -db assembly -format docsum | xtract -pattern DocumentSummary -if RefSeq -unless Sub_value -element SpeciesName RefSeq FtpPath_RefSeq | sed "s/GCF_/NA\tGCF_/" >> {output}' -- {{}}
        """
        
rule calculate_ani:
    conda:
        "env/ani.yaml"
    input:
        "refSeq/genomes/downloads.txt",
        "local_data/ids.txt"
    output:
        "ANI/log.txt"
    shell:
        """
        cp refSeq/genomes/*.fna $(dirname {output})
        cp $(dirname {input[1]})/genomes/*.fna $(dirname {output})
        average_nucleotide_identity.py -i $(dirname {output}) -o $( dirname {output})/results/ -l {output} -g --gformat eps,png -f
        """

rule extract_16S_from_local:
    input:
        "{genome}/annotation/{genome}.ffn"
    output:
        "genes/{genome}_16S.fa"
    shell:
        "sed -n -e  \"/16S ribosomal RNA/,/^>/ p\" {input} | sed \$d | sed \"s/PROKKA_[0-9]\+/{wildcards.genome}/\"| sed \"s/ /_/g\" > {output[0]}"

rule merge_16S_from_local:
    input:
        expand("genes/{genome}_16S.fa", genome=samples)
    output:
        "genes/merged_local_16S.fa"
    shell:
        "cat {input} > {output}"

        
rule blast_16S:
    conda:
        "env/blast.yaml"
    input:
        "genes/merged_local_16S.fa"
    output:
        "genes/16S_filtered.txt"
    shell:
        "~/Téléchargements/ncbi-blast-2.6.0+/bin/blastn -remote -db nt -query {input[0]} -outfmt \"6 sgi sstart send staxids\" | awk '$4!=\"155900\"' | awk '$4!=\"77133\"' > {output[0]}" 
        
rule fetch_16S_fasta:
    conda:
        "env/entrez-direct.yaml"
    input:
        "genes/16S_filtered.txt"
    output:
        "genes/16S_filtered.fasta"
    shell:
        """
        while read p;
        do
            gi=$(echo ${p} | cut -d' ' -f1)
            start=$(echo ${p} | cut -d' ' -f2)
            end=$(echo ${p} | cut -d' ' -f3)
            efetch -db nuccore -format fasta -id ${gi} -chr_start ${start} -chr_stop ${end} | sed "s/ /_/g" | sed "s/://g" | sed "s/,//g" | sed "s/(//g" | sed "s/)//g"  >> {output[0]}
        done < {input[0]}
        """

        
        
rule unzip_genomes:
    input:
        "refSeq/genomes/log_wget.txt"
    output:
        "refSeq/genomes/log_unzip.txt"
    shell:
        """
        for i in $( ls $( dirname {input})/*.gz)
        do 
            gzip -d -f $i
        done > {output}
        """

rule orthofinder_downloaded_proteomes:
    conda:
        "env/ortho_search.yaml"
    input:
        "refSeq/proteomes/downloads.txt"
    output:
        "orthologs/current_results/Orthogroups.csv"
    log:
        "orthologs/log.txt"
    shell:
        "rm -rf $(dirname {input})/Results_* && rm -rf $(dirname {output})/ && orthofinder -oa -M msa -f $(dirname {input}) > {log} && mv $(dirname {input})/Results_* $(dirname {output}) && mv {log} $(dirname {output})/log.txt"



rule adding_local_genomes_to_orthofinder_run:
    conda:
        "env/ortho_search.yaml"
    input:
        ancient("orthologs/current_results/Orthogroups.csv"),
        "local_data/ids.txt"
    output:
        "orthologs/current_results/id.txt",
        "orthologs/SpeciesIDs.txt",
        "orthologs/current_results/log.txt"
    log:
        "orthologs/log.txt"
    shell:
        """
        if [ ! "$(find $(dirname {input[1]})/proteomes/to_be_added/ -name *.faa)" ]
        then
            echo 'No additional fasta files included for new orthofinder run'
            cp $(dirname {log})/tmp/id.txt {output[0]}
            cp $(dirname {log})/tmp/log.txt {output[2]}
            sed "s/://" $(dirname {log})/tmp/WorkingDirectory/SpeciesIDs.txt | sed "s/\\.faa//" | sed "s/^#.*//" > {output[1]}
        else
            rm -rf $(dirname {log})/tmp/
            cp -R $(dirname {input[0]}) $(dirname {log})/tmp/ 
            orthofinder -oa -M msa -b $(dirname {log})/tmp/WorkingDirectory/ -f $(dirname {input[1]})/proteomes/to_be_added/ > {log}
            grep -no "Statistics_Overall_[0-9]\\+.csv" {log} | sed "s/.*\\([0-9]\\+\\).csv/\\1/" > $(dirname {log})/tmp/id.txt
            if [ -s $(dirname {log})/tmp/id.txt ]
            then
                rm -rf $(dirname {input[0]})
                mv {log} $(dirname {log})/tmp/log.txt
                cp -R $(dirname {log})/tmp/ $(dirname {input[0]})
                sed "s/://" $(dirname {output[0]})/WorkingDirectory/SpeciesIDs.txt | sed "s/\\.faa//" | sed "s/^#.*//" | sed "s/\[/{{/" | sed "s/\]/}}/" > {output[1]}
                mkdir -p $(dirname {input[1]})/proteomes/added/
                mv $(dirname {input[1]})/proteomes/to_be_added/*.faa $(dirname {input[1]})/proteomes/added/

            fi
        fi
        """

rule extracting_alignments_one_to_one_orthologs:
    input:
        "orthologs/current_results/id.txt",
        "orthologs/current_results/log.txt"
    output:
        "orthologs/1to1groups/log.txt"
    shell:
        """
        out_folder=$(dirname {output[0]})
        if [ "$(ls ${{out_folder}})" ]
        then
            rm ${{out_folder}}/*
        fi
        run_number=$(cat {input[0]})
        alignmentsid_folder=$(grep -A 1 "Multiple sequence alignments:" {input[1]}  | tail -n 1 | sed "s/Alignments/WorkingDirectory\/Alignments_ids/" | sed "s/_tmp/_current_proteomes/")
        single_copy_orthogroups=$(dirname {input[1]})/WorkingDirectory/SingleCopyOrthogroups_${{run_number}}.txt
        echo ${{out_folder}} ${{run_number}} ${{alignmentsid_folder}} ${{single_copy_orthogroups}}
        for og in $(cat ${{single_copy_orthogroups}})
        do
            awk '/^>/ {{printf("\\n%s\\n",$0);next; }} {{ printf("%s",$0);}}  END {{printf("\\n");}}' < ${{alignmentsid_folder}}/${{og}}.fa >  ${{out_folder}}/${{og}}_oneline.fa;
        done
        sed -i "s/>\([0-9]\+\)_.*/>\\1/" ${{out_folder}}/*_oneline.fa
        sed -i "/^\s*$/d" ${{out_folder}}/*_oneline.fa
        echo "Orthogroups extracted" >> {output}
        """

rule prepare_raxml_files:
    input:
        "orthologs/1to1groups/log.txt"
    output:
        "phylogeny/concatenated_orthogroups.fa",
        "phylogeny/partition_file.txt"
    shell:
        """
        target_folder=$(dirname {input})
        type=prot
        model=LG
        index=0
        start=1
        for i in $(ls ${{target_folder}}/*_oneline.fa)
        do
            length=$(tail -n -1 ${{i}} | tr -d '\\n' | wc -m);
            echo "${{model}}, ${{type}}${{index}} = ${{start}}-$(( start + length - 1 ))" >> {output[1]}
            start=$(( start + length ));
            index=$(( index + 1 ));
        done
        paste -d'\\0' ${{target_folder}}/*_oneline.fa | sed "s/>\([0-9]\+\)>.*/>\\1/"  |  sed "/^\s*$/d" | sed "s/U/C/g" > {output[0]}
        """
        #sed "s/U/C/g" replaces Selenocysteine (U) which is not accepted by raxml by Cysteine (C) which is the closest AA possible

rule run_raxml_on_1to1groups:
    conda:
        "env/raxml.yaml"
    input:
        "phylogeny/concatenated_orthogroups.fa",
        "phylogeny/partition_file.txt"
    output:
        "phylogeny/RAxML_info.run",
        "phylogeny/RAxML_bestTree.run"
    shell:
        """
        raxmlHPC -m PROTGAMMALG -w $(dirname $(readlink -f {output[0]})) -n run -s {input[0]} -q {input[1]} -p 123 > $(dirname {output[0]})/log.txt
        """

rule generate_tree_figure_from_raxml:
    conda:
        "env/tree_figure.yaml"
    input:
        "phylogeny/RAxML_bestTree.run",
        "orthologs/SpeciesIDs.txt"
    output:
        "phylogeny/tree.svg",
        "phylogeny/tree.png"
    shell:
        """
        nw_rename {input[0]} {input[1]} | nw_display -S -s -b 'opacity:0' -w 800 - > {output[0]}
        convert {output[0]} {output[1]}
        """
        
        
def extract_orthofinder_results(filename):
    s=""
    stats_file_pattern = re.compile(r"Statistics_Overall_\d+\.csv")
    folder_pattern = re.compile(r"existing analysis in /.*")
    stats_file = re.findall(stats_file_pattern, open(filename, "r").read())
    folder = re.findall(folder_pattern, open(filename, "r").read())
    return "\n    ".join(open(("/"+"/".join(folder[0].split("/")[1:])+stats_file[0]), "r").read().split("Date")[0].split("\n"))
        




rule report:
    input:
        species="refSeq/nb_genomes_per_species.txt",
        genomes="refSeq/genome_addresses.txt",
        local_genomes="local_data/ids.txt",
        ortho="orthologs/current_results/log.txt",
        pyani="ANI/log.txt",
        tree="phylogeny/tree.png"
    output:
        "report.html"
    run:
        tree=input["tree"]
        queries="\n    ".join(genus)
        numbers="\n    ".join(open(input["species"], "r").read().split("\n"))
        genomes="\n    ".join(open(input["genomes"], "r").read().split("\n"))
        idlocal="\n    ".join(open(input["local_genomes"], "r").read().split("\n"))
        report_ortho=extract_orthofinder_results(input["ortho"])
        report("""
        Report of the comparative genomics pipeline 
        ===========================================
        The RefSeq database was searched for the available genomes using the requested queries::
        
            {queries}

        Number of available genomes in RefSeq::
        
            {numbers}

        If species had more than 5 genomes available, only the reference and representative genomes were selected. Hence here are the genomes that were downloaded for this analysis::
            
            Species name\t Subspecies name\t RefSeqID \t FTP link
            {genomes}
        
        Additionnaly, here are the local genomes that were included in this analysis::
        
            {idlocal}

        Orthofinder was run on those genomes and the output statistics were::

            {report_ortho}
        
        A phylogeny was constructed using the single-copy orthogroups using RAxML:

        .. image:: {tree}

        The Average Nucleotide Identity (ANI) between all genomes was calculated using pyani. Here are the ANI values:

        .. image:: ANI/results/ANIm_percentage_identity.png
        
        And here is the percentage of the genome that was aligned:

        .. image:: ANI/results/ANIm_alignment_coverage.png
        """, output[0], **input)
        
