rule get_mentalist_mlst_scheme:
    conda:
        pipeline_path + "envs/mentalist.yml"
    params:
        species_name=lambda wildcards, output: output["database"].split('/')[-2].replace("_", " "),
    output:
        database="references/mentalist/{species}/mlst.db",
        profiles="references/mentalist/{species}/mlst.db.profile",
    shell:
        """
        mentalist download_pubmlst -k 31 -o $(dirname {output.profiles})/mlst_fasta_files/ -s "{params.species_name}" --db {output.database}
        """

rule determine_mlst_from_trimmed_reads:
    conda:
        pipeline_path + "envs/mentalist.yml"
    input:
        fastq1 = "samples/{sample}/reads/trimmed/R1_paired.fastq",
        fastq2 = "samples/{sample}/reads/trimmed/R2_paired.fastq",
    output:
        "samples/{sample}/typing/mentalist.txt",
    params:
        species=lambda wildcards: all_samples.loc[wildcards.sample, "ScientificName"].replace(" ", "_"),
    log:
        logging_folder+"samples/{sample}/logs/typing/mentalist/log.txt"
    shell:
        """
        mentalist call -o {output[0]} --db references/mentalist/{params.species}/mlst.db -1 {input.fastq1} -2 {input.fastq2} >> {log[0]}
        """

rule merge_mentalist_from_all_samples:
    conda:
        pipeline_path + "envs/mlst.yml"
    input:
        mlsts = expand("samples/{sample}/typing/mentalist.txt", sample=read_naming.keys())
    params:
        samples=list(read_naming.keys())
    output:
        "typing/mlst/mentalist_summary.tsv"
    shell:
        """
        for i in {params.samples}; do
            cat samples/$i/typing/mentalist.txt | sed "s/^R/$i/" >> {output}.tmp
        done
        # remove duplicated header
        awk '!x[$0]++' {output}.tmp > {output}
        rm {output}.tmp
        """